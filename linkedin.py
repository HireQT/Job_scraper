# -*- coding: utf-8 -*-
"""linkedin_scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EpKfWb39pbjF_Lff5Ss411NHacDaOwV0

"""

#Webdriver and web scraping
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup as  bs4
import csv
from jobspy import scrape_jobs
import requests
import json

#For dropdown menu of options
import ipywidgets as widgets
from IPython.display import display

# Set the path to the ChromeDriver executable
chrome_options = Options()
chrome_options.add_argument('--headless=new')
chrome_options.headless = True
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('--disable-extensions')
chrome_options.add_argument('--disable-infobars')
chrome_options.add_argument('--disable-features=NetworkService')
chrome_options.add_argument('--no-sandbox')

# Create a new instance of the Chrome browser
driver = webdriver.Chrome(options=chrome_options)

# Selecting the user options for job search
job_role_dropdown = widgets.Dropdown(
    options=['None','App Development', 'Data Science', 'Deep Learning'],
    value='None',
    description='Job Role:',
    disabled=False,
)

job_type_dropdown = widgets.Dropdown(
    options=['None','Internship', 'Fulltime'],
    value='None',
    description='Job Type:',
    disabled=False,
)

def on_change(change):
    if change['type'] == 'change' and change['name'] == 'value':
        print(f"Selected: {change['new']}")

job_role_dropdown.observe(on_change)
display(job_role_dropdown)
job_type_dropdown.observe(on_change)
display(job_type_dropdown)

#Applying user choices to job search
job_role = job_role_dropdown.value
job_type = job_type_dropdown.value
year = '2024'
country = 'India'

job_role = job_role.replace(' ','+')
job_search = f'{job_role}+{job_type}+{year}+{country}'.lower()
website = f"https://www.google.com/search?q={job_search}&ibp=htl;jobs#htivrt=jobs"
website

import time
# Navigate to a web page
driver.get(website)
driver.implicitly_wait(100)
time.sleep(5)

soup=bs4.BeautifulSoup(driver.page_source,"html.parser")
#import requests
#response = requests.get(website)
#soup = BeautifulSoup(driver.page_source,'html.parser')


jobs = scrape_jobs(
    site_name=["indeed", "linkedin", "glassdoor","internshala"],
    search_term="Data Science Intern",
    location = "India",
    results_wanted=20,
    hours_old=168, # (only Linkedin/Indeed is hour specific, others round up to days old)
    country_indeed='India'  # only needed for indeed / glassdoor
)
print(f"Found {len(jobs)} jobs")
print(jobs.head())
jobs.to_csv("jobs.csv", quoting=csv.QUOTE_NONNUMERIC, escapechar="\\", index=False)


# Define the URL and parameters
url = "https://api.scrapingdog.com/linkedinjobs/"
api_key = "65ec8d1d17a1b774efc65b86"
field = "software%20engineer%20AI"
geoid = "101165590"
page = 4

all_jobs = []

# Define parameters for each page
params = {
    "api_key": api_key,
    "field": field,
    "geoid": geoid,
    "page": str(page)
}

# Send a GET request with the parameters
response = requests.get(url, params=params)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Access the response content
    all_jobs.extend(response.json())
else:
    print("Request failed with status code:", response.status_code)


# Base URL and API key
base_url = "https://api.scrapingdog.com/linkedinjobs"
api_key = "65ec8d1d17a1b774efc65b86"

# Iterate through all jobs
for job in all_jobs:
    # Extract job ID
    job_id = job['job_id']

    # Define parameters
    params = {
        "api_key": api_key,
        "job_id": job_id
    }

    # Send request to get job details
    response = requests.get(base_url, params=params)

    # Check if request was successful
    if response.status_code == 200:
        # Parse JSON response
        job_details = response.json()[0]

        # Add job details to the original job dictionary
        job.update(job_details)
    else:
        print(f"Failed to fetch details for job with ID {job_id}")

# Now all_jobs contains detailed information for each job

